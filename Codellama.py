import torch
from transformers import CodellamaForConditionalGeneration, CodellamaTokenizer
import gradio as gr

# Load Codellama model and tokenizer
model_name = "codellama/34b"
tokenizer = CodellamaTokenizer.from_pretrained(model_name)
model = CodellamaForConditionalGeneration.from_pretrained(model_name)

# Function to generate code from input prompt
def generate_code(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    # Ensure model runs on multiple GPUs if available
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
    model.to("cuda")
    output = model.generate(input_ids.to("cuda"))
    generated_code = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated_code

# Create Gradio interface
iface = gr.Interface(
    fn=generate_code,
    inputs=gr.inputs.Textbox(lines=10, label="Input Prompt"),
    outputs=gr.outputs.Textbox(label="Generated Code"),
    title="Codellama 34b Code Generator",
    description="Enter a text prompt and get code generated by Codellama 34b.",
)

# Launch the interface
iface.launch()
